{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, numpy as np, pandas as pd\n",
    "\n",
    "def load_config(path='default_config.yaml', **ov):\n",
    "    with open(path) as f: cfg = yaml.safe_load(f)\n",
    "    cfg.update(ov)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()  # or: load_config(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 folds, test set: 504 days\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Data Loading & Cross-Validation Splits\n",
    "# =============================================================================\n",
    "# Expanding window validation for financial time series\n",
    "# Avoids look-ahead bias and tests across multiple market regimes\n",
    "# Reference: de Prado (2018) \"Advances in Financial Machine Learning\", Ch. 7\n",
    "\n",
    "def create_folds(data, test_years=2, init_train_years=10, val_years=1, step_years=1):\n",
    "    \"\"\"\n",
    "    Expanding window cross-validation for time series.\n",
    "    \n",
    "    Returns:\n",
    "        folds: list of (train_df, val_df) tuples\n",
    "        test: held-out test set (final test_years of data)\n",
    "    \"\"\"\n",
    "    days_per_year = 252  # trading days\n",
    "    \n",
    "    # Hold out final test set\n",
    "    test_size = test_years * days_per_year\n",
    "    test = data.iloc[-test_size:]\n",
    "    remaining = data.iloc[:-test_size]\n",
    "    \n",
    "    # Create expanding folds\n",
    "    folds = []\n",
    "    train_end = init_train_years * days_per_year\n",
    "    val_size = val_years * days_per_year\n",
    "    step = step_years * days_per_year\n",
    "    \n",
    "    while train_end + val_size <= len(remaining):\n",
    "        train = remaining.iloc[:train_end]\n",
    "        val = remaining.iloc[train_end:train_end + val_size]\n",
    "        folds.append((train, val))\n",
    "        train_end += step\n",
    "    \n",
    "    return folds, test\n",
    "\n",
    "data = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n",
    "folds, test = create_folds(data)\n",
    "print(f\"Created {len(folds)} folds, test set: {len(test)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Weights Sharpe: 1.645\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Equal Weights Baseline\n",
    "# =============================================================================\n",
    "# 1/N portfolio - surprisingly hard to beat\n",
    "# Reference: DeMiguel, Garlappi & Uppal (2009) \"Optimal Versus Naive \n",
    "#            Diversification\", Review of Financial Studies\n",
    "\n",
    "w = np.ones(test.shape[1]) / test.shape[1]\n",
    "r = (test * w).sum(axis=1)\n",
    "ew_sharpe = r.mean() / r.std() * np.sqrt(252)\n",
    "print(f\"Equal Weights Sharpe: {ew_sharpe:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: DDPG / DDES-DDPG Runner\n",
    "# =============================================================================\n",
    "# DDPG: Lillicrap et al. (2015) \"Continuous control with deep reinforcement learning\"\n",
    "# DDES: Hong et al. (2018) \"Diversity-Driven Exploration Strategy for Deep RL\"\n",
    "#\n",
    "# Key difference:\n",
    "#   DDPG  - exploration via Gaussian noise on actions\n",
    "#   DDES  - exploration via diversity term in actor loss: -Q(s,a) + Î± * D(a, a_prior)\n",
    "\n",
    "from models.div_ddpg import DDPG\n",
    "from models.networks import NeuralNetwork\n",
    "\n",
    "def run_ddpg(train, val, test, cfg, use_ddes=False):\n",
    "    \"\"\"Train and evaluate DDPG or DDES-DDPG.\"\"\"\n",
    "    agent = DDPG(\n",
    "        lookback_window=cfg['lookback_window'],\n",
    "        predictor=NeuralNetwork,\n",
    "        batch_size=cfg['batch_size'],\n",
    "        hidden_sizes=cfg['hidden_sizes'],\n",
    "        seed=cfg['seeds'][0],\n",
    "    )\n",
    "    agent.train(\n",
    "        train, val,\n",
    "        actor_lr=cfg['actor_lr'],\n",
    "        critic_lr=cfg['critic_lr'],\n",
    "        gamma=cfg['gamma'],\n",
    "        tau=cfg['tau'],\n",
    "        soft_update=cfg['soft_update'],\n",
    "        num_epochs=cfg['num_epochs'],\n",
    "        patience=cfg['patience'],\n",
    "        noise=cfg['noise'],\n",
    "        use_ddes=use_ddes,\n",
    "        ddes_alpha=cfg['ddes_alpha'],\n",
    "    )\n",
    "    _, (_, sharpe) = agent.evaluate(test)\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 ---\n",
      "Train: 2520 days, Val: 252 days\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(folds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days, Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mDDPG\u001b[39m\u001b[33m'\u001b[39m].append(\u001b[43mrun_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_ddes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     11\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mDDES\u001b[39m\u001b[33m'\u001b[39m].append(run_ddpg(train, val, test, cfg, use_ddes=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_ddpg\u001b[39m\u001b[34m(train, val, test, cfg, use_ddes)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ddpg\u001b[39m(train, val, test, cfg, use_ddes=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train and evaluate DDPG or DDES-DDPG.\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m     agent = DDPG(\n\u001b[32m     17\u001b[39m         lookback_window=cfg[\u001b[33m'\u001b[39m\u001b[33mlookback_window\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     18\u001b[39m         predictor=NeuralNetwork,\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         batch_size=\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     20\u001b[39m         hidden_sizes=cfg[\u001b[33m'\u001b[39m\u001b[33mhidden_sizes\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     21\u001b[39m         seed=cfg[\u001b[33m'\u001b[39m\u001b[33mseeds\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     agent.train(\n\u001b[32m     24\u001b[39m         train, val,\n\u001b[32m     25\u001b[39m         actor_lr=cfg[\u001b[33m'\u001b[39m\u001b[33mactor_lr\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m         ddes_alpha=cfg[\u001b[33m'\u001b[39m\u001b[33mddes_alpha\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     _, (_, sharpe) = agent.evaluate(test)\n",
      "\u001b[31mKeyError\u001b[39m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Run Experiments Across Folds\n",
    "# =============================================================================\n",
    "results = {'EW': ew_sharpe, 'DDPG': [], 'DDES': []}\n",
    "\n",
    "for i, (train, val) in enumerate(folds):\n",
    "    print(f\"\\n--- Fold {i+1}/{len(folds)} ---\")\n",
    "    print(f\"Train: {len(train)} days, Val: {len(val)} days\")\n",
    "    \n",
    "    results['DDPG'].append(run_ddpg(train, val, test, cfg, use_ddes=False))\n",
    "    results['DDES'].append(run_ddpg(train, val, test, cfg, use_ddes=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Method        Mean Sharpe        Std\n",
      "--------------------------------------------------\n",
      "EW                  1.645          -\n",
      "DDPG                  nan        nan\n",
      "DDES                  nan        nan\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:222: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Results Summary\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"{'Method':<12} {'Mean Sharpe':>12} {'Std':>10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'EW':<12} {results['EW']:>12.3f} {'-':>10}\")\n",
    "print(f\"{'DDPG':<12} {np.mean(results['DDPG']):>12.3f} {np.std(results['DDPG']):>10.3f}\")\n",
    "print(f\"{'DDES':<12} {np.mean(results['DDES']):>12.3f} {np.std(results['DDES']):>10.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Using cached cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: cloudpickle\n",
      "Successfully installed cloudpickle-3.1.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install cloudpickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Imports + Config\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "from models.pga_map_elites.official_networks import Actor, Critic\n",
    "from models.pga_map_elites.official_utils import (\n",
    "    ReplayBuffer, Individual, add_to_archive, cvt\n",
    ")\n",
    "from models.pga_map_elites.official_variational_operators import VariationalOperator\n",
    "from models.pga_map_elites.portfolio_env import PortfolioEnv\n",
    "\n",
    "cfg = {\n",
    "    'seeds': [42],\n",
    "    'hidden_sizes': [128, 128],\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.005,\n",
    "    'actor_lr': 0.001,\n",
    "    'n_niches': 128,\n",
    "    'max_evals': 5000,\n",
    "    'random_init': 200,\n",
    "    'batch_size': 100,\n",
    "    'proportion_evo': 0.5,\n",
    "    'iso_sigma': 0.005,\n",
    "    'line_sigma': 0.05,\n",
    "    'nr_of_steps_crit': 300,\n",
    "    'nr_of_steps_act': 10,\n",
    "    'train_batch_size': 256,\n",
    "    'policy_noise': 0.2,\n",
    "    'noise_clip': 0.5,\n",
    "    'policy_freq': 2,\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: eval_policy (only function not in utils)\n",
    "# =============================================================================\n",
    "def eval_policy(actor, env):\n",
    "    \"\"\"Run one episode. Source: parallel_worker() in original PGA-MAP-Elites.\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    states, actions, next_states, rewards, not_dones = [], [], [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        action = actor.select_action(np.array(state))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        done_bool = float(done) if env._step < env._max_episode_steps else 0\n",
    "        \n",
    "        states.append(state.copy())\n",
    "        actions.append(action.copy())\n",
    "        next_states.append(next_state.copy())\n",
    "        rewards.append(reward)\n",
    "        not_dones.append(1.0 - done_bool)\n",
    "        state = next_state\n",
    "    \n",
    "    transitions = (\n",
    "        np.array(states), np.array(actions), np.array(next_states),\n",
    "        np.array(rewards).reshape(-1,1), np.array(not_dones).reshape(-1,1)\n",
    "    )\n",
    "    return env.tot_reward, env.desc.copy(), transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: using cached CVT: /Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/CVT/centroids_128_2.dat\n",
      "Mutation operator: False\n",
      "Crossover operator: <bound method VariationalOperator.iso_dd of <models.pga_map_elites.official_variational_operators.VariationalOperator object at 0x165e00830>>\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Setup\n",
    "# =============================================================================\n",
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "env = PortfolioEnv(data.iloc[:-504], lookback=20, episode_len=50)\n",
    "\n",
    "torch.manual_seed(cfg['seeds'][0])\n",
    "np.random.seed(cfg['seeds'][0])\n",
    "\n",
    "critic = Critic(env.state_dim, env.action_dim, max_action=1.0,\n",
    "                discount=cfg['gamma'], tau=cfg['tau'],\n",
    "                policy_noise=cfg['policy_noise'], noise_clip=cfg['noise_clip'])\n",
    "replay_buffer = ReplayBuffer(env.state_dim, env.action_dim)\n",
    "kdt = KDTree(cvt(cfg['n_niches'], dim=2, samples=25000))\n",
    "archive = {}\n",
    "\n",
    "# Variation operator (uses your existing iso_dd and pg_variation)\n",
    "var_op = VariationalOperator(\n",
    "    actor_fn=lambda: Actor(env.state_dim, env.action_dim, 1.0, cfg['hidden_sizes']),\n",
    "    num_cpu=4,\n",
    "    iso_sigma=cfg['iso_sigma'],\n",
    "    line_sigma=cfg['line_sigma'],\n",
    "    learning_rate=cfg['actor_lr'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/official_variational_operators.py\", line 64, in parallel_worker\n",
      "    actor_loss = -critic.Q1(state, actor_z(state)).mean()\n",
      "                  ^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Critic' object has no attribute 'Q1'\n",
      "  File \"/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/official_variational_operators.py\", line 64, in parallel_worker\n",
      "    actor_loss = -critic.Q1(state, actor_z(state)).mean()\n",
      "                  ^^^^^^^^^\n",
      "AttributeError: 'Critic' object has no attribute 'Q1'\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/official_variational_operators.py\", line 64, in parallel_worker\n",
      "    actor_loss = -critic.Q1(state, actor_z(state)).mean()\n",
      "                  ^^^^^^^^^\n",
      "AttributeError: 'Critic' object has no attribute 'Q1'\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/official_variational_operators.py\", line 64, in parallel_worker\n",
      "    actor_loss = -critic.Q1(state, actor_z(state)).mean()\n",
      "                  ^^^^^^^^^\n",
      "AttributeError: 'Critic' object has no attribute 'Q1'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m         states = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     offspring = \u001b[43mvar_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproportion_evo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_of_steps_act\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnr_of_steps_act\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m actor \u001b[38;5;129;01min\u001b[39;00m offspring:\n\u001b[32m     22\u001b[39m     fitness, desc, transitions = eval_policy(actor, env)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/pga_map_elites/official_variational_operators.py:219\u001b[39m, in \u001b[36mVariationalOperator.__call__\u001b[39m\u001b[34m(self, archive, batch_size, proportion_evo, critic, states, train_batch_size, nr_of_steps_act)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m.actors_train_in_queue.put((n,\n\u001b[32m    213\u001b[39m                                     actors_x_grad[n].x,\n\u001b[32m    214\u001b[39m                                     critic,\n\u001b[32m    215\u001b[39m                                     states,\n\u001b[32m    216\u001b[39m                                     nr_of_steps_act))\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(actors_x_grad)):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     n, actor_z = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactors_train_out_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     actors_z_grad[n] = actor_z\n\u001b[32m    221\u001b[39m actors_z += actors_z_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:101\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._rlock:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m._sem.release()\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:216\u001b[39m, in \u001b[36m_ConnectionBase.recv_bytes\u001b[39m\u001b[34m(self, maxlength)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength < \u001b[32m0\u001b[39m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mnegative maxlength\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m._bad_message_length()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:395\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    393\u001b[39m remaining = size\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 4: Main loop\n",
    "# =============================================================================\n",
    "n_evals = 0\n",
    "\n",
    "while n_evals < cfg['max_evals']:\n",
    "    \n",
    "    if n_evals < cfg['random_init']:\n",
    "        offspring = [Actor(env.state_dim, env.action_dim, 1.0, cfg['hidden_sizes']) \n",
    "                     for _ in range(cfg['batch_size'])]\n",
    "    else:\n",
    "        if replay_buffer.size > cfg['train_batch_size']:\n",
    "            critic.train(archive, replay_buffer, cfg['nr_of_steps_crit'], cfg['train_batch_size'])\n",
    "            states = replay_buffer.sample_state(cfg['train_batch_size'], cfg['nr_of_steps_act'])\n",
    "        else:\n",
    "            states = None\n",
    "        \n",
    "        offspring = var_op(archive, cfg['batch_size'], cfg['proportion_evo'],\n",
    "                          critic=critic, states=states, nr_of_steps_act=cfg['nr_of_steps_act'])\n",
    "    \n",
    "    for actor in offspring:\n",
    "        fitness, desc, transitions = eval_policy(actor, env)\n",
    "        replay_buffer.add(transitions)\n",
    "        add_to_archive(Individual(actor, desc, fitness), desc, archive, kdt)\n",
    "    \n",
    "    n_evals += len(offspring)\n",
    "    \n",
    "    if n_evals % 500 == 0:\n",
    "        best = max((x.fitness for x in archive.values()), default=0)\n",
    "        print(f\"[{n_evals}] Archive: {len(archive)} | Best: {best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 5: Results\n",
    "# =============================================================================\n",
    "best_key = max(archive.keys(), key=lambda k: archive[k].fitness)\n",
    "print(f\"Best fitness: {archive[best_key].fitness:.4f}\")\n",
    "print(f\"Coverage: {100*len(archive)/cfg['n_niches']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
