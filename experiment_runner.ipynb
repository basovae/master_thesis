{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Optimization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_manager import Config\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4527, 42)\n",
      "Date range: 2007-12-20 00:00:00 to 2025-12-18 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('data.csv', index_col=0, parse_dates=True)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set `RUN_MODE` to switch between quick test and full overnight run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODE = 'overnight'  # 'quick' or 'overnight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run mode: overnight\n",
      "PGA iterations: 1000\n",
      "Seeds: [42, 123, 456]\n"
     ]
    }
   ],
   "source": [
    "if RUN_MODE == 'quick':\n",
    "    # Quick test - few minutes to check everything works\n",
    "    cfg = Config(\n",
    "        validation={\n",
    "            'strategy': 'holdout',\n",
    "            'n_seeds': 1,\n",
    "            'random_seeds': [42]\n",
    "        },\n",
    "        ddpg={\n",
    "            'total_timesteps': 5000,\n",
    "            'warmup_steps': 500,\n",
    "            'buffer_size': 10000\n",
    "        },\n",
    "        div_ddpg={\n",
    "            'total_timesteps': 5000,\n",
    "            'warmup_steps': 500,\n",
    "            'buffer_size': 10000\n",
    "        },\n",
    "        pga_map_elites={\n",
    "            'n_iterations': 20,\n",
    "            'archive': {'n_niches': 50},\n",
    "            'initial_population': 10,\n",
    "            'batch_size_eval': 8\n",
    "        },\n",
    "        logging={'log_freq': 100, 'eval_freq': 500}\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    cfg = Config(\n",
    "        validation={\n",
    "            'strategy': 'expanding_window',\n",
    "            'n_seeds': 3,\n",
    "            'random_seeds': [42, 123, 456]\n",
    "        },\n",
    "        ddpg={\n",
    "            'num_epochs': 500,\n",
    "            'patience': 20,\n",
    "            'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'batch_size': 64,\n",
    "            'tau': 0.005,\n",
    "            'gamma': 0.99,\n",
    "            'hidden_sizes' :[64, 64],\n",
    "            'verbose' : 1\n",
    "        },\n",
    "        div_ddpg={\n",
    "            'num_epochs': 500,\n",
    "            'patience': 20,\n",
    "            'actor_lr': 0.0001,\n",
    "            'critic_lr': 0.001,\n",
    "            'alpha': 5.0,  # stronger diversity pressure\n",
    "            'scaling_method': 'fixed'\n",
    "        },\n",
    "        pga_map_elites={\n",
    "            'max_evals': 20000,\n",
    "            'n_niches': 512,\n",
    "            'random_init': 500,\n",
    "            'eval_batch_size': 100\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Run mode: {RUN_MODE}\")\n",
    "#print(f\"DDPG timesteps: {cfg.ddpg.total_timesteps}\")\n",
    "print(f\"PGA iterations: {cfg.pga_map_elites.n_iterations}\")\n",
    "print(f\"Seeds: {cfg.validation.random_seeds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3168 days (2007-12-20 to 2020-07-22)\n",
      "Val: 679 days (2020-07-23 to 2023-04-03)\n",
      "Test: 680 days (2023-04-04 to 2025-12-18)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"Simple holdout split.\"\"\"\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train = data.iloc[:train_end]\n",
    "    val = data.iloc[train_end:val_end]\n",
    "    test = data.iloc[val_end:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "print(f\"Train: {len(train_data)} days ({train_data.index[0].date()} to {train_data.index[-1].date()})\")\n",
    "print(f\"Val: {len(val_data)} days ({val_data.index[0].date()} to {val_data.index[-1].date()})\")\n",
    "print(f\"Test: {len(test_data)} days ({test_data.index[0].date()} to {test_data.index[-1].date()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "seed = cfg.validation.random_seeds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Equal Weights Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Weights - Sharpe: 0.528\n"
     ]
    }
   ],
   "source": [
    "# Equal weights - no training needed\n",
    "n_assets = train_data.shape[1]\n",
    "ew_weights = np.ones(n_assets) / n_assets\n",
    "\n",
    "# Evaluate on validation set\n",
    "ew_returns = (val_data * ew_weights).sum(axis=1)\n",
    "results['equal_weights'] = {\n",
    "    'cumulative_return': (1 + ew_returns).prod() - 1,\n",
    "    'sharpe': ew_returns.mean() / ew_returns.std() * np.sqrt(252),\n",
    "    'weights': ew_weights\n",
    "}\n",
    "print(f\"Equal Weights - Sharpe: {results['equal_weights']['sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1280) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetworks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralNetwork\n\u001b[32m      4\u001b[39m ddpg_agent = DDPG(\n\u001b[32m      5\u001b[39m     lookback_window=cfg.data.lookback_window,\n\u001b[32m      6\u001b[39m     predictor=NeuralNetwork,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     hidden_sizes=cfg.ddpg.actor_hidden_dims\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mddpg_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactor_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcritic_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoft_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mddpg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpatience\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     28\u001b[39m spo_results, dpo_results = ddpg_agent.evaluate(test_data, dpo=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/ddpg.py:153\u001b[39m, in \u001b[36mDDPG.train\u001b[39m\u001b[34m(self, train_data, val_data, actor_lr, critic_lr, optimizer, l1_lambda, l2_lambda, soft_update, tau, risk_preference, weight_decay, gamma, num_epochs, early_stopping, patience, min_delta)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# run training loop\u001b[39;00m\n\u001b[32m    135\u001b[39m trainer = DDPGTrainer(\n\u001b[32m    136\u001b[39m     number_of_assets=\u001b[38;5;28mself\u001b[39m.number_of_assets,\n\u001b[32m    137\u001b[39m     actor=\u001b[38;5;28mself\u001b[39m.actor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m     min_delta=min_delta,\n\u001b[32m    152\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/ddpg.py:26\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, train_loader, val_loader, num_epochs, noise, verbose)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDDPG\u001b[39;00m:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Meta-class for Deep Determinitic Policy Gradient Reinforcement Learning.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m        lookback_window (int): The size of the lookback window for input data.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m        predictor (predictors): The predictor class to use for the model.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m        batch_size (int, optional): The number of samples per batch. Defaults\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m            to 1.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m        short_selling (bool, optional): Whether to allow short selling, i.e.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[33;03m            negative portfolio weights in the model. Defaults to False.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m        forecast_window (int, optional): The size of the forecast window for\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m            input data. Defaults to 0.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m        reduce_negatives (bool, optional): Whether to clamp negative portfolio\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m            weights to -100 %. Defaults to False.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m        verbose (int, optional): The verbosity level for logging and outputs.\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m            Defaults to 1.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m        seed (int, optional): Random seed for reproducibility. Defaults to 42.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m        **kwargs: Keyword arguments to be passed to the predictor at init.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     38\u001b[39m         lookback_window: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m         **kwargs,\n\u001b[32m     47\u001b[39m     ):\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mself\u001b[39m.lookback_window = lookback_window\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1280) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from models.ddpg import DDPG\n",
    "from models.networks import NeuralNetwork\n",
    "\n",
    "ddpg_agent = DDPG(\n",
    "    lookback_window=cfg.data.lookback_window,\n",
    "    predictor=NeuralNetwork,\n",
    "    batch_size=cfg.ddpg.batch_size,\n",
    "    short_selling=False,\n",
    "    verbose=1,\n",
    "    seed=seed,\n",
    "    hidden_sizes=cfg.ddpg.actor_hidden_dims\n",
    ")\n",
    "\n",
    "ddpg_agent.train(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    actor_lr=cfg.ddpg.actor_lr,\n",
    "    critic_lr=cfg.ddpg.critic_lr,\n",
    "    tau=cfg.ddpg.tau,\n",
    "    gamma=cfg.ddpg.gamma,\n",
    "    soft_update=True,\n",
    "    num_epochs=cfg.ddpg.num_epochs,\n",
    "    early_stopping=False,\n",
    "    patience=cfg.ddpg.patience\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "spo_results, dpo_results = ddpg_agent.evaluate(test_data, dpo=True)\n",
    "\n",
    "results['ddpg'] = {\n",
    "    'spo_profit': spo_results[0],\n",
    "    'spo_sharpe': spo_results[1],\n",
    "    'dpo_profit': dpo_results[0],\n",
    "    'dpo_sharpe': dpo_results[1]\n",
    "}\n",
    "print(f\"DDPG - SPO Sharpe: {results['ddpg']['spo_sharpe']:.3f}, DPO Sharpe: {results['ddpg']['dpo_sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Div-DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Actor: 0.067764, Critic: 0.037132, Diversity: -0.000000, Alpha: 1.0000, Val: 0.022819\n",
      "Epoch 2/500, Actor: 0.032715, Critic: 0.012844, Diversity: -0.000004, Alpha: 1.0000, Val: 0.045447\n",
      "Epoch 3/500, Actor: 0.053467, Critic: 0.005136, Diversity: -0.000034, Alpha: 1.0000, Val: 0.031672\n",
      "Epoch 4/500, Actor: 0.059809, Critic: 0.006661, Diversity: -0.000183, Alpha: 1.0000, Val: 0.036120\n",
      "Epoch 5/500, Actor: -0.007309, Critic: 0.004476, Diversity: -0.001485, Alpha: 0.9998, Val: 0.056933\n",
      "Epoch 6/500, Actor: 0.042062, Critic: 0.005319, Diversity: -0.005309, Alpha: 0.9985, Val: 0.039361\n",
      "Epoch 7/500, Actor: 0.060838, Critic: 0.008194, Diversity: -0.006219, Alpha: 0.9947, Val: 0.036847\n",
      "Epoch 8/500, Actor: 0.025450, Critic: 0.006844, Diversity: -0.002705, Alpha: 0.9938, Val: 0.074271\n",
      "Epoch 9/500, Actor: 0.033072, Critic: 0.003919, Diversity: -0.007063, Alpha: 0.9973, Val: 0.074149\n",
      "Epoch 10/500, Actor: 0.022796, Critic: 0.004080, Diversity: -0.014258, Alpha: 0.9930, Val: 0.032578\n",
      "Epoch 11/500, Actor: 0.070523, Critic: 0.006403, Diversity: -0.008489, Alpha: 0.9859, Val: 0.039116\n",
      "Epoch 12/500, Actor: 0.030463, Critic: 0.006572, Diversity: -0.008135, Alpha: 0.9916, Val: 0.055486\n",
      "Epoch 13/500, Actor: 0.041691, Critic: 0.008180, Diversity: -0.006872, Alpha: 0.9919, Val: 0.065346\n",
      "Epoch 14/500, Actor: 0.040641, Critic: 0.006259, Diversity: -0.004366, Alpha: 0.9932, Val: 0.063497\n",
      "Epoch 15/500, Actor: 0.041072, Critic: 0.007075, Diversity: -0.004302, Alpha: 0.9957, Val: 0.053638\n",
      "Epoch 16/500, Actor: 0.021756, Critic: 0.005650, Diversity: -0.004246, Alpha: 0.9957, Val: 0.064755\n",
      "Epoch 17/500, Actor: -0.011875, Critic: 0.005253, Diversity: -0.003962, Alpha: 0.9958, Val: 0.062215\n",
      "Epoch 18/500, Actor: 0.022584, Critic: 0.007991, Diversity: -0.004224, Alpha: 0.9961, Val: 0.046359\n",
      "Epoch 19/500, Actor: 0.023875, Critic: 0.005585, Diversity: -0.004499, Alpha: 0.9958, Val: 0.039007\n",
      "Epoch 20/500, Actor: 0.039317, Critic: 0.007532, Diversity: -0.004062, Alpha: 0.9955, Val: 0.060495\n",
      "Epoch 21/500, Actor: 0.000885, Critic: 0.007765, Diversity: -0.003756, Alpha: 0.9960, Val: 0.058492\n",
      "Early stopping triggered.\n",
      "XLK        1.10 %\n",
      "XLF        0.03 %\n",
      "XLV        0.98 %\n",
      "XLE        3.23 %\n",
      "XLI        0.15 %\n",
      "XLP        3.74 %\n",
      "XLY        0.02 %\n",
      "XLU        0.02 %\n",
      "XLB        3.91 %\n",
      "SPY        0.03 %\n",
      "IWM        0.02 %\n",
      "MDY        0.22 %\n",
      "VTV        4.12 %\n",
      "VUG        0.13 %\n",
      "EFA        4.21 %\n",
      "EEM        0.02 %\n",
      "VGK        2.24 %\n",
      "EWJ        0.05 %\n",
      "FXI        6.14 %\n",
      "EWY        2.20 %\n",
      "EWZ        0.05 %\n",
      "EWG        0.16 %\n",
      "EWU        1.05 %\n",
      "AGG        3.05 %\n",
      "TLT        3.41 %\n",
      "IEF        7.71 %\n",
      "SHY        7.38 %\n",
      "LQD        0.06 %\n",
      "HYG        0.15 %\n",
      "MUB        0.61 %\n",
      "TIP        8.94 %\n",
      "EMB        1.28 %\n",
      "BND        7.10 %\n",
      "GLD        8.41 %\n",
      "SLV        4.87 %\n",
      "VNQ        9.72 %\n",
      "DBA        0.10 %\n",
      "GSG        0.04 %\n",
      "IAU        0.81 %\n",
      "GDX        0.05 %\n",
      "QQQ        2.17 %\n",
      "VWO        0.33 %\n",
      "\n",
      "Portfolio Allocation (SPO):\n",
      "XLK        1.10 %\n",
      "XLF        0.03 %\n",
      "XLV        0.98 %\n",
      "XLE        3.23 %\n",
      "XLI        0.15 %\n",
      "XLP        3.74 %\n",
      "XLY        0.02 %\n",
      "XLU        0.02 %\n",
      "XLB        3.91 %\n",
      "SPY        0.03 %\n",
      "IWM        0.02 %\n",
      "MDY        0.22 %\n",
      "VTV        4.12 %\n",
      "VUG        0.13 %\n",
      "EFA        4.21 %\n",
      "EEM        0.02 %\n",
      "VGK        2.24 %\n",
      "EWJ        0.05 %\n",
      "FXI        6.14 %\n",
      "EWY        2.20 %\n",
      "EWZ        0.05 %\n",
      "EWG        0.16 %\n",
      "EWU        1.05 %\n",
      "AGG        3.05 %\n",
      "TLT        3.41 %\n",
      "IEF        7.71 %\n",
      "SHY        7.38 %\n",
      "LQD        0.06 %\n",
      "HYG        0.15 %\n",
      "MUB        0.61 %\n",
      "TIP        8.94 %\n",
      "EMB        1.28 %\n",
      "BND        7.10 %\n",
      "GLD        8.41 %\n",
      "SLV        4.87 %\n",
      "VNQ        9.72 %\n",
      "DBA        0.10 %\n",
      "GSG        0.04 %\n",
      "IAU        0.81 %\n",
      "GDX        0.05 %\n",
      "QQQ        2.17 %\n",
      "VWO        0.33 %\n",
      "\n",
      "Profit p.a. (SPO): 12.7493 %\n",
      "Sharpe Ratio (SPO): 0.0893\n",
      "\n",
      "Performing dynamic portfolio optimization over 35 intervals...\n",
      "\n",
      " |██████████████████████████████| 100.0%  [Elapsed: 0.0s]\n",
      "\n",
      "Profit p.a. (DPO): 12.9748 %\n",
      "Sharpe Ratio (DPO): 0.0895\n",
      "\n",
      "Div-DDPG - SPO Sharpe: 0.089, DPO Sharpe: 0.090\n"
     ]
    }
   ],
   "source": [
    "from models.div_ddpg import DivDDPG\n",
    "from models.networks import NeuralNetwork\n",
    "\n",
    "div_ddpg_agent = DivDDPG(\n",
    "    lookback_window=cfg.data.lookback_window,\n",
    "    predictor=NeuralNetwork,\n",
    "    batch_size=cfg.div_ddpg.batch_size,\n",
    "    short_selling=False,\n",
    "    verbose=1,\n",
    "    seed=seed,\n",
    "    hidden_sizes=cfg.div_ddpg.actor_hidden_dims\n",
    ")\n",
    "\n",
    "div_ddpg_agent.train(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    actor_lr=cfg.div_ddpg.actor_lr,\n",
    "    critic_lr=cfg.div_ddpg.critic_lr,\n",
    "    tau=cfg.div_ddpg.tau,\n",
    "    gamma=cfg.div_ddpg.gamma,\n",
    "    soft_update=cfg.div_ddpg.soft_update,\n",
    "    num_epochs=cfg.div_ddpg.num_epochs,\n",
    "    early_stopping=cfg.div_ddpg.early_stopping,\n",
    "    patience=cfg.div_ddpg.patience,\n",
    "    # Diversity params\n",
    "    alpha=cfg.div_ddpg.diversity.alpha,\n",
    "    alpha_final=cfg.div_ddpg.diversity.alpha_final,\n",
    "    scaling_method=cfg.div_ddpg.diversity.scaling_method,\n",
    "    n_prior_samples=cfg.div_ddpg.diversity.n_prior_samples\n",
    ")\n",
    "\n",
    "spo_results, dpo_results = div_ddpg_agent.evaluate(test_data, dpo=True)\n",
    "\n",
    "results['div_ddpg'] = {\n",
    "    'spo_profit': spo_results[0],\n",
    "    'spo_sharpe': spo_results[1],\n",
    "    'dpo_profit': dpo_results[0],\n",
    "    'dpo_sharpe': dpo_results[1]\n",
    "}\n",
    "print(f\"Div-DDPG - SPO Sharpe: {results['div_ddpg']['spo_sharpe']:.3f}, DPO Sharpe: {results['div_ddpg']['dpo_sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PGA-MAP-Elites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PGA-MAP-Elites Initialization\n",
      "============================================================\n",
      "Computing CVT with 256 niches, 2D behavior space...\n",
      "WARNING: using cached CVT: /Users/ekaterinabasova/Desktop/untitled folder/master_thesis/models/pga_map_elites/CVT/centroids_256_2.dat\n",
      "  CVT computed in 0.0s\n",
      "  Replay buffer initialized (max size: 1,000,000)\n",
      "  Critic initialized (TD3-style twin critics)\n",
      "\n",
      "Config Summary:\n",
      "  Max evaluations: 1,000\n",
      "  Random init: 200 evals\n",
      "  Batch size: 50\n",
      "  Variation split: 50% GA, 50% PG\n",
      "  Critic training: 300 steps/iter\n",
      "  PG variation: 10 steps/offspring\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Iteration 1 | Evals: 0/1000\n",
      "============================================================\n",
      "Phase: Random Initialization (0/200)\n",
      "\n",
      "  [Evaluation] Evaluating 50 policies...\n",
      "    Evaluated 25/50...\n",
      "    Evaluated 50/50...\n",
      "    Time: 0.1s\n",
      "    Batch fitness: mean=0.0134, max=0.1352, min=-0.1779\n",
      "\n",
      "  [Summary]\n",
      "    Iteration time: 0.2s (total: 0.2s)\n",
      "  Archive Metrics:\n",
      "    Coverage: 5/256 (2.0%)\n",
      "    Fitness: max=0.1352, mean=0.0401, median=0.0699, std=0.1150\n",
      "    QD-Score: 0.2004\n",
      "    BD Distribution:\n",
      "      volatility: mean=0.133, std=0.113, range=[0.059, 0.358]\n",
      "      diversification: mean=0.867, std=0.025, range=[0.822, 0.895]\n",
      "  Replay Buffer: 2,500/1,000,000 (0.2%)\n",
      "  Random policies added: 15/50\n",
      "  *** New best fitness: 0.1352 ***\n",
      "\n",
      "============================================================\n",
      "Iteration 2 | Evals: 50/1000\n",
      "============================================================\n",
      "Phase: Random Initialization (50/200)\n",
      "\n",
      "  [Evaluation] Evaluating 50 policies...\n",
      "    Evaluated 25/50...\n",
      "    Evaluated 50/50...\n",
      "    Time: 0.2s\n",
      "    Batch fitness: mean=0.0257, max=0.1853, min=-0.0493\n",
      "\n",
      "  [Summary]\n",
      "    Iteration time: 0.3s (total: 0.5s)\n",
      "  Archive Metrics:\n",
      "    Coverage: 9/256 (3.5%)\n",
      "    Fitness: max=0.1853, mean=0.0693, median=0.1060, std=0.1008\n",
      "    QD-Score: 0.6234\n",
      "    BD Distribution:\n",
      "      volatility: mean=0.147, std=0.111, range=[0.026, 0.358]\n",
      "      diversification: mean=0.881, std=0.038, range=[0.820, 0.937]\n",
      "  Replay Buffer: 5,000/1,000,000 (0.5%)\n",
      "  Random policies added: 8/50\n",
      "  *** New best fitness: 0.1853 ***\n",
      "\n",
      "============================================================\n",
      "Iteration 3 | Evals: 100/1000\n",
      "============================================================\n",
      "Phase: Random Initialization (100/200)\n",
      "\n",
      "  [Evaluation] Evaluating 50 policies...\n",
      "    Evaluated 25/50...\n",
      "    Evaluated 50/50...\n",
      "    Time: 0.2s\n",
      "    Batch fitness: mean=0.0211, max=0.1079, min=-0.1092\n",
      "\n",
      "  [Summary]\n",
      "    Iteration time: 0.3s (total: 0.8s)\n",
      "  Archive Metrics:\n",
      "    Coverage: 12/256 (4.7%)\n",
      "    Fitness: max=0.1853, mean=0.0681, median=0.1054, std=0.0922\n",
      "    QD-Score: 0.8176\n",
      "    BD Distribution:\n",
      "      volatility: mean=0.180, std=0.124, range=[0.026, 0.402]\n",
      "      diversification: mean=0.882, std=0.035, range=[0.820, 0.937]\n",
      "  Replay Buffer: 7,500/1,000,000 (0.8%)\n",
      "  Random policies added: 3/50\n",
      "\n",
      "============================================================\n",
      "Iteration 4 | Evals: 150/1000\n",
      "============================================================\n",
      "Phase: Random Initialization (150/200)\n",
      "\n",
      "  [Evaluation] Evaluating 50 policies...\n",
      "    Evaluated 25/50...\n",
      "    Evaluated 50/50...\n",
      "    Time: 0.2s\n",
      "    Batch fitness: mean=0.0167, max=0.0776, min=-0.2200\n",
      "\n",
      "  [Summary]\n",
      "    Iteration time: 0.3s (total: 1.1s)\n",
      "  Archive Metrics:\n",
      "    Coverage: 14/256 (5.5%)\n",
      "    Fitness: max=0.1853, mean=0.0464, median=0.0884, std=0.1089\n",
      "    QD-Score: 0.6494\n",
      "    BD Distribution:\n",
      "      volatility: mean=0.186, std=0.131, range=[0.031, 0.402]\n",
      "      diversification: mean=0.886, std=0.033, range=[0.820, 0.937]\n",
      "  Replay Buffer: 10,000/1,000,000 (1.0%)\n",
      "  Random policies added: 3/50\n",
      "\n",
      "============================================================\n",
      "Iteration 5 | Evals: 200/1000\n",
      "============================================================\n",
      "Phase: Selection & Variation\n",
      "\n",
      "  [Critic Training]\n",
      "    [Critic] Adding 14 new species to training pool\n",
      "    [Critic] Training 300 steps with 14 actors\n",
      "      Step 100/300: Critic Loss=0.0004, Avg Q=-0.0429\n",
      "      Step 200/300: Critic Loss=0.0002, Avg Q=-0.0438\n",
      "      Step 300/300: Critic Loss=0.0001, Avg Q=-0.0427\n",
      "    [Critic] Training complete:\n",
      "      Avg Critic Loss: 0.0002\n",
      "      Avg Q-value: -0.0431\n",
      "      Avg Actor Loss: 0.0096\n",
      "      Actors in pool: 14\n",
      "    Time: 15.6s\n",
      "    Final Critic Loss: 0.0002\n",
      "    Avg Q-value: -0.0431\n",
      "\n",
      "  [PG Variation] Generating 25 offspring...\n",
      "    Generated 10/25 PG offspring...\n",
      "    Generated 20/25 PG offspring...\n",
      "    Time: 1.0s\n",
      "\n",
      "  [GA Variation] Generating 25 offspring...\n",
      "    Time: 0.1s\n",
      "\n",
      "  [Evaluation] Evaluating 50 policies...\n",
      "    Evaluated 25/50...\n",
      "    Evaluated 50/50...\n",
      "    Time: 0.2s\n",
      "    Batch fitness: mean=0.0145, max=0.1472, min=-0.2846\n",
      "\n",
      "  [Summary]\n",
      "    Iteration time: 16.9s (total: 18.0s)\n",
      "  Archive Metrics:\n",
      "    Coverage: 31/256 (12.1%)\n",
      "    Fitness: max=0.1853, mean=0.0287, median=0.0337, std=0.0977\n",
      "    QD-Score: 0.8908\n",
      "    BD Distribution:\n",
      "      volatility: mean=0.136, std=0.112, range=[0.031, 0.402]\n",
      "      diversification: mean=0.587, std=0.312, range=[0.049, 0.937]\n",
      "  Replay Buffer: 12,500/1,000,000 (1.2%)\n",
      "  Variation Success:\n",
      "    PG: 21/25 (84.0%)\n",
      "    GA: 2/25 (8.0%)\n",
      "    Total: 23/50 (46.0%)\n",
      "\n",
      "============================================================\n",
      "Iteration 6 | Evals: 250/1000\n",
      "============================================================\n",
      "Phase: Selection & Variation\n",
      "\n",
      "  [Critic Training]\n",
      "    [Critic] Adding 17 new species to training pool\n",
      "    [Critic] Training 300 steps with 31 actors\n",
      "      Step 100/300: Critic Loss=0.0001, Avg Q=-0.0375\n",
      "      Step 200/300: Critic Loss=0.0001, Avg Q=-0.0305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m config[\u001b[33m\"\u001b[39m\u001b[33meval_batch_size\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m50\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m archive = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/pga_map_elites/pga_map_elites.py:457\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(env, config)\u001b[39m\n\u001b[32m    454\u001b[39m critic_start = time.time()\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# [LOGGING ADDED] Use verbose mode for critic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m critic_metrics = \u001b[43mcritic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnr_of_steps_crit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_batch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m critic_time = time.time() - critic_start\n\u001b[32m    466\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/pga_map_elites/networks.py:272\u001b[39m, in \u001b[36mCritic.train\u001b[39m\u001b[34m(self, archive, replay_buffer, nr_of_steps, batch_size, verbose)\u001b[39m\n\u001b[32m    266\u001b[39m noise = (\n\u001b[32m    267\u001b[39m     torch.randn_like(action) * \u001b[38;5;28mself\u001b[39m.policy_noise\n\u001b[32m    268\u001b[39m ).clamp(-\u001b[38;5;28mself\u001b[39m.noise_clip, \u001b[38;5;28mself\u001b[39m.noise_clip)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, actor_target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.actor_targets):\n\u001b[32m    271\u001b[39m     next_action = (\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[43mactor_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m + noise\n\u001b[32m    273\u001b[39m     ).clamp(-\u001b[38;5;28mself\u001b[39m.max_action, \u001b[38;5;28mself\u001b[39m.max_action)\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Clipped double Q-learning: use min of Q1, Q2\u001b[39;00m\n\u001b[32m    276\u001b[39m     target_Q1, target_Q2 = \u001b[38;5;28mself\u001b[39m.critic_target(next_state, next_action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/models/pga_map_elites/networks.py:88\u001b[39m, in \u001b[36mActor.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_action * torch.tanh(\u001b[38;5;28mself\u001b[39m.n3(\u001b[38;5;28mself\u001b[39m.l3(a)))\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     a = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     89\u001b[39m     a = F.relu(\u001b[38;5;28mself\u001b[39m.l2(a))\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_action * torch.tanh(\u001b[38;5;28mself\u001b[39m.l3(a))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/untitled folder/master_thesis/.venv/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models.pga_map_elites.try_pga_out import PortfolioEnv\n",
    "from models.pga_map_elites.pga_map_elites import main, config\n",
    "\n",
    "# Create environment with your data split\n",
    "env = PortfolioEnv(\n",
    "    data_path=\"data.csv\",  # or pass the dataframe directly\n",
    "    lookback=cfg.data.lookback_window,\n",
    "    episode_len=50\n",
    ")\n",
    "\n",
    "# Update config\n",
    "config[\"state_dim\"] = env.state_dim\n",
    "config[\"action_dim\"] = env.action_dim\n",
    "config[\"eval_batch_size\"] = 20\n",
    "config[\"max_evals\"] = 1000\n",
    "config[\"random_init\"] = 200\n",
    "config[\"eval_batch_size\"] = 50\n",
    "\n",
    "\n",
    "# Run\n",
    "archive = main(env, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best policy: fitness=0.2651, BD=[0.20122131 0.        ]\n",
      "PGA-MAP-Elites - Sharpe: 0.672, Return: 25.7%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Get best policy from archive\n",
    "best_key = max(archive.keys(), key=lambda k: archive[k].fitness)\n",
    "best_policy = archive[best_key].x\n",
    "best_fitness = archive[best_key].fitness\n",
    "best_bd = archive[best_key].desc\n",
    "\n",
    "print(f\"Best policy: fitness={best_fitness:.4f}, BD={best_bd}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_env = PortfolioEnv(data_path=\"data.csv\", lookback=cfg.data.lookback_window, episode_len=len(test_data)-cfg.data.lookback_window-1)\n",
    "\n",
    "# Run evaluation\n",
    "state = test_env.reset()\n",
    "test_returns = []\n",
    "test_weights = []\n",
    "\n",
    "for _ in range(len(test_data) - cfg.data.lookback_window - 1):\n",
    "    with torch.no_grad():\n",
    "        action = best_policy(torch.FloatTensor(state).unsqueeze(0)).cpu().numpy().flatten()\n",
    "    weights = np.abs(action) / (np.sum(np.abs(action)) + 1e-8)\n",
    "    test_weights.append(weights)\n",
    "    \n",
    "    next_state, reward, done, info = test_env.step(action)\n",
    "    test_returns.append(info['portfolio_return'])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "test_returns = np.array(test_returns)\n",
    "pga_sharpe = np.mean(test_returns) / (np.std(test_returns) + 1e-8) * np.sqrt(252)\n",
    "pga_cum_return = (1 + test_returns).prod() - 1\n",
    "\n",
    "results['pga_map_elites'] = {\n",
    "    'sharpe': pga_sharpe,\n",
    "    'cumulative_return': pga_cum_return,\n",
    "    'coverage': len(archive) / config['n_niches'],\n",
    "    'qd_score': sum(ind.fitness for ind in archive.values())\n",
    "}\n",
    "\n",
    "print(f\"PGA-MAP-Elites - Sharpe: {pga_sharpe:.3f}, Return: {pga_cum_return*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equal_weights: Sharpe=0.528, Return=17.2%\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "for method, res in results.items():\n",
    "    if 'sharpe' in res:\n",
    "        print(f\"{method}: Sharpe={res['sharpe']:.3f}, Return={res['cumulative_return']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS SUMMARY\n",
      "==================================================\n",
      "equal_weights       : Sharpe=0.528\n",
      "ddpg                : SPO Sharpe=0.066, DPO Sharpe=0.068\n",
      "div_ddpg            : SPO Sharpe=0.080, DPO Sharpe=0.080\n",
      "pga_map_elites      : Sharpe=0.672\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for method, res in results.items():\n",
    "    if 'sharpe' in res:\n",
    "        print(f\"{method:20s}: Sharpe={res['sharpe']:.3f}\")\n",
    "    elif 'spo_sharpe' in res:\n",
    "        print(f\"{method:20s}: SPO Sharpe={res['spo_sharpe']:.3f}, DPO Sharpe={res['dpo_sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config for reproducibility\n",
    "cfg.save(f'experiments/{RUN_MODE}_config.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
